{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hMcXhArbQhoF1sZ5FfqMj0le68vSOK3g",
      "authorship_tag": "ABX9TyPauvw8Mm8+ZoabeH3PYagJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davydantoniuk/grammarfix-bot/blob/main/model_gramma_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SprcdvKVNdDy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "df =pd.read_excel('/results.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "eZp2DRVqOCqJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization and prepering data**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VvDvMD_2Nwhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = tokenizer(text, padding='max_length', max_length=50, truncation=True, return_tensors='pt')\n",
        "    return tokens.input_ids, tokens.attention_mask\n",
        "\n",
        "input_tokens = tokenize_text(df['Altered'].tolist())\n",
        "target_tokens = tokenize_text(df['Original'].tolist())\n",
        "\n",
        "print(\"Входные токены с ошибками (Altered):\", input_tokens[1])\n",
        "print(\"Целевые токены без ошибок (Original):\", target_tokens[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eveNdE-qRIMi",
        "outputId": "a2f0eee7-0869-4ffd-b425-5f2981f04e65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Входные токены с ошибками (Altered): tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "Целевые токены без ошибок (Original): tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a model"
      ],
      "metadata": {
        "id": "kLXqoUmHYX2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Seq2SeqModel(nn.Module):\n",
        "  def __init__(self,vocab_size,embedding_dim=256,hidden_dim=512,num_layers=2):\n",
        "    super(Seq2SeqModel,self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size , embedding_dim)\n",
        "    self.encoder =nn.LSTM(embedding_dim,hidden_dim , num_layers,batch_first = True)\n",
        "    self.decoder = nn.LSTM(embedding_dim,hidden_dim , num_layers,batch_first = True)\n",
        "    self.fc = nn.Linear(hidden_dim,vocab_size)\n",
        "  def forward(self,input_ids,target_ids,attention_mask=None):\n",
        "    embedded = self.embedding(input_ids)\n",
        "    encoder_outputs , (hidden,cell) = self.encoder(embedded)\n",
        "    target_embedded = self.embedding(target_ids)\n",
        "    decoder_outputs, _ = self.decoder(target_embedded,(hidden,cell))\n",
        "    output = self.fc(decoder_outputs)\n",
        "    return output"
      ],
      "metadata": {
        "id": "7sbuOT2UTdkm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing data for learning"
      ],
      "metadata": {
        "id": "sCOnOpzEiRwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self,df,tokinizer , max_length = 50):\n",
        "    self.df = df\n",
        "    self.tokenizer = tokinizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    input_text = self.df.iloc[idx]['Original']\n",
        "    target_text = self.df.iloc[idx]['Altered']\n",
        "\n",
        "    input_encoded = self.tokenizer(\n",
        "        input_text,\n",
        "        padding='max_length',\n",
        "        max_length = self.max_length,\n",
        "        truncation=True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "    target_encoded = self.tokenizer(\n",
        "        target_text,\n",
        "        padding = 'max_length',\n",
        "        max_length = self.max_length,\n",
        "        truncation = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "\n",
        "    input_ids = input_encoded['input_ids'].squeeze()\n",
        "    target_ids = target_encoded['input_ids'].squeeze()\n",
        "\n",
        "    return input_ids,target_ids\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dataset = TextDataset(df,tokenizer)\n",
        "data_loader =DataLoader(dataset,batch_size=32,shuffle=True)\n",
        "\n",
        "#Trying to extract one batch\n",
        "\n",
        "for input_ids,target_ids in data_loader:\n",
        "  print('Input IDs:',input_ids)\n",
        "  print('Target IDs:',target_ids)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8-piSxSiDm6",
        "outputId": "b0120ce1-655d-4238-97e4-9d671c5f3e8c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[    0,   970,    16,  ...,     1,     1,     1],\n",
            "        [    0,   243,  1382,  ...,     1,     1,     1],\n",
            "        [    0,    17,    48,  ...,     1,     1,     1],\n",
            "        ...,\n",
            "        [    0,  3908,    10,  ...,     1,     1,     1],\n",
            "        [    0,  1106,    24,  ...,     1,     1,     1],\n",
            "        [    0,  2387, 14706,  ...,     1,     1,     1]])\n",
            "Target IDs: tensor([[   0,  970,   16,  ...,    1,    1,    1],\n",
            "        [   0,  243, 1382,  ...,    1,    1,    1],\n",
            "        [   0,   17,   48,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   0, 3908,   10,  ...,    1,    1,    1],\n",
            "        [   0, 1106,   24,  ...,    1,    1,    1],\n",
            "        [   0, 2387,  910,  ...,    1,    1,    1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCD4LsGlKUH2",
        "outputId": "65c22113-3801-4016-e4d4-5b2d64d1dcfe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M62_K1ouKXgD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}